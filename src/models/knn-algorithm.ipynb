{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Paths\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "train_path = os.path.join(current_dir, \"../data/train.csv\")\n",
    "test_path = os.path.join(current_dir, \"../data/test.csv\")\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_df, test_df=None, test_size=0.3, random_state=42):\n",
    "    X = train_df.drop(['label', 'id', 'FILENAME', 'URL', 'Domain'], axis=1)\n",
    "    y = train_df['label']\n",
    "\n",
    "    numeric_columns = X.select_dtypes(include=['number']).columns\n",
    "    categorical_columns = X.select_dtypes(exclude=['number']).columns\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        X[col] = np.log1p(X[col])\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        X[col] = X[col].fillna(X[col].median())\n",
    "\n",
    "    for col in categorical_columns:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "    label_encoders = {}\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    smote = SMOTE(random_state=random_state, k_neighbors=1)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "    if test_df is not None:\n",
    "        X_test_final = test_df.drop(['id', 'FILENAME', 'URL', 'Domain'], axis=1)\n",
    "        for col in numeric_columns:\n",
    "            X_test_final[col] = np.log1p(X_test_final[col])\n",
    "            X_test_final[col] = X_test_final[col].fillna(X_test_final[col].median())\n",
    "        for col in categorical_columns:\n",
    "            X_test_final[col] = X_test_final[col].fillna(X_test_final[col].mode()[0])\n",
    "            X_test_final[col] = X_test_final[col].map(lambda val: label_encoders[col].transform([val])[0]\n",
    "                                                      if val in label_encoders[col].classes_\n",
    "                                                      else -1)\n",
    "        X_test_final[numeric_columns] = scaler.transform(X_test_final[numeric_columns])\n",
    "    else:\n",
    "        X_test_final = None\n",
    "\n",
    "    return X_train_resampled, X_test, y_train_resampled, y_test, X_test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class KNNClassifier:\n",
    "    def __init__(self, k=3, metric=\"euclidean\", batch_size=1000, n_threads=4):\n",
    "        \"\"\"\n",
    "        Initialize the KNN Classifier with thread pool support.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        k : int, default=3\n",
    "            Number of neighbors to use\n",
    "        metric : str, default=\"euclidean\"\n",
    "            Distance metric to use\n",
    "        batch_size : int, default=1000\n",
    "            Number of test points to process in each batch\n",
    "        n_threads : int, default=4\n",
    "            Number of threads to use for parallel processing\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.metric = metric\n",
    "        self.batch_size = batch_size\n",
    "        self.n_threads = n_threads\n",
    "        self.train_data = None\n",
    "        self.train_labels = None\n",
    "\n",
    "    def fit(self, train_data, train_labels):\n",
    "        \"\"\"\n",
    "        Store the training data and labels.\n",
    "        \"\"\"\n",
    "        self.train_data = np.asarray(train_data, dtype=np.float32)\n",
    "        self.train_labels = np.asarray(train_labels)\n",
    "        return self\n",
    "\n",
    "    def _compute_batch_distances(self, test_batch):\n",
    "        \"\"\"\n",
    "        Compute distances for a batch of test points.\n",
    "        \"\"\"\n",
    "        if self.metric == 'euclidean':\n",
    "            distances = cdist(test_batch, self.train_data, metric='euclidean')\n",
    "        elif self.metric == 'manhattan':\n",
    "            distances = cdist(test_batch, self.train_data, metric='cityblock')\n",
    "        elif self.metric == 'minkowski':\n",
    "            distances = cdist(test_batch, self.train_data, metric='minkowski')\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {self.metric}\")\n",
    "        return distances\n",
    "\n",
    "    def _predict_batch(self, test_batch):\n",
    "        \"\"\"\n",
    "        Predict labels for a batch of test points.\n",
    "        \"\"\"\n",
    "        distances = self._compute_batch_distances(test_batch)\n",
    "        batch_predictions = []\n",
    "\n",
    "        for point_distances in distances:\n",
    "            k_indices = np.argpartition(point_distances, self.k)[:self.k]\n",
    "            k_labels = self.train_labels[k_indices]\n",
    "\n",
    "            prediction = Counter(k_labels).most_common(1)[0][0]\n",
    "            batch_predictions.append(prediction)\n",
    "\n",
    "        return batch_predictions\n",
    "\n",
    "    def predict(self, test_points):\n",
    "        \"\"\"\n",
    "        Predict labels for test points using a thread pool.\n",
    "        \"\"\"\n",
    "        if self.train_data is None:\n",
    "            raise ValueError(\"Model has not been trained. Call 'fit' first.\")\n",
    "\n",
    "        test_points = np.asarray(test_points, dtype=np.float32)\n",
    "        predictions = []\n",
    "\n",
    "        # Split test points into batches\n",
    "        batches = [\n",
    "            test_points[i:i + self.batch_size]\n",
    "            for i in range(0, len(test_points), self.batch_size)\n",
    "        ]\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor(max_workers=self.n_threads) as executor:\n",
    "            results = executor.map(self._predict_batch, batches)\n",
    "\n",
    "        for result in results:\n",
    "            predictions.extend(result)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {\n",
    "            'k': self.k,\n",
    "            'metric': self.metric,\n",
    "            'batch_size': self.batch_size,\n",
    "            'n_threads': self.n_threads\n",
    "        }\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        if 'k' in params:\n",
    "            self.k = params['k']\n",
    "        if 'metric' in params:\n",
    "            self.metric = params['metric']\n",
    "        if 'batch_size' in params:\n",
    "            self.batch_size = params['batch_size']\n",
    "        if 'n_threads' in params:\n",
    "            self.n_threads = params['n_threads']\n",
    "        return self\n",
    "\n",
    "    def score(self, X_test, y_test):\n",
    "        predictions = self.predict(X_test)\n",
    "        return np.mean(predictions == y_test)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        \"\"\"\n",
    "        Save the model to a file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(filename):\n",
    "        \"\"\"\n",
    "        Load the model from a file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "        print(f\"Model loaded from {filename}\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9756000513412912\n",
      "ROC AUC Score: 0.9756000513412912\n",
      "Recall: 0.9756000513412912\n",
      "Precision: 0.9756005781872705\n",
      "F1 Score: 0.9756000445840358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score\n",
    "import pickle\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Preprocess data\n",
    "X_train, X_test, y_train, y_test, scaler, label_encoders = preprocess_data(train_df)\n",
    "\n",
    "# Train the KNN model\n",
    "knn = KNNClassifier(k=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "roc_auc = roc_auc_score(y_test, predictions, multi_class='ovr', average='macro')\n",
    "recall = recall_score(y_test, predictions, average='macro')\n",
    "precision = precision_score(y_test, predictions, average='macro')\n",
    "f1 = f1_score(y_test, predictions, average='macro')\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"ROC AUC Score: {roc_auc}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'submission.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Preprocess the train and test data\n",
    "X_train, X_test_split, y_train, y_test_split, X_test = preprocess_data(train_df, test_df)\n",
    "\n",
    "# Train the KNN model\n",
    "knn = KNNClassifier(k=3, metric='euclidean')\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the KNN model\n",
    "predictions = knn.predict(X_test)\n",
    "\n",
    "# Save predictions in the required format\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test_df[\"id\"],  # Use the 'id' column from the test data\n",
    "    \"label\": predictions   # Add the predicted labels\n",
    "})\n",
    "\n",
    "# Specify the file path for saving\n",
    "submission_file_path = 'submission.csv'\n",
    "submission.to_csv(submission_file_path, index=False)\n",
    "\n",
    "print(f\"Predictions saved to '{submission_file_path}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
